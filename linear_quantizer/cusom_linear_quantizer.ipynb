{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4-A - Building your own Quantizer: Custom Build an 8-Bit Quantizer\n",
    "\n",
    "In this lesson, you will learn how to compress any model in 8-bit precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - `w8_a16_forward` Function\n",
    "\n",
    "-\n",
    "```Python\n",
    "W8A16LinearLayer\n",
    "                    # 8-bit  # 16-bit         # optional\n",
    "* w8_a16_forward -> weights, input,   scales, bias=None\n",
    "                    \n",
    "```\n",
    "- Cast the 8-bit `weights` to the same data type as the `input`, \"casted weights\",\n",
    "- keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
    "- Next, $$(({inputs} \\cdot \\text{``casted weights''}) * {scale}) + {bias}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hidden_state = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "random_scale = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication between hidden states and random weights:  tensor([[-326.0000,   57.5000, -414.0000,  376.0000,   68.5000, -102.5000,\n",
      "          -52.0000,  468.0000, -232.0000,  179.0000,  116.5000, -149.0000,\n",
      "         -102.5000,   99.0000, -127.0000,  314.0000,  150.0000, -191.0000,\n",
      "         -112.5000,   97.5000, -100.5000, -201.0000,   29.0000,  178.0000,\n",
      "          104.5000,  111.5000, -194.0000,   98.5000,  173.0000,  239.0000,\n",
      "         -131.0000, -326.0000]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# perform matrix multiplication\n",
    "matrix_mul = F.linear(random_hidden_state, random_int8.to(random_hidden_state.dtype))\n",
    "print(\"Matrix Multiplication between hidden states and random weights: \", matrix_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication with scale:  tensor([[-7.1000e+01, -3.4000e+01,  2.5400e+02,  1.2900e+02,  4.1211e-01,\n",
      "          4.8000e+01, -6.3000e+01, -4.5200e+02,  3.0800e+02, -1.0500e+02,\n",
      "         -5.8250e+01, -1.5400e+02,  9.7500e+01,  5.9062e+00, -1.7000e+02,\n",
      "         -2.0400e+02, -8.8125e+00,  3.1800e+02, -1.6000e+02,  1.0650e+02,\n",
      "          1.6800e+02,  2.5300e+02,  1.6250e+01, -1.6500e+02,  8.8000e+01,\n",
      "          3.8250e+01,  1.4000e+02,  2.0200e+02, -3.0600e+02, -2.0900e+02,\n",
      "         -1.4400e+02,  6.0800e+02]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Multiply the result with the scale \n",
    "matrix_scale = F.linear(random_hidden_state, random_int8.to(random_hidden_state.dtype)) * random_scale\n",
    "print(\"Matrix Multiplication with scale: \", matrix_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication with scale and bias:  tensor([[-7.1500e+01, -3.5000e+01,  2.5300e+02,  1.2900e+02, -1.7773e-01,\n",
      "          4.7750e+01, -6.3250e+01, -4.5200e+02,  3.0800e+02, -1.0500e+02,\n",
      "         -5.6000e+01, -1.5500e+02,  9.8000e+01,  3.7969e+00, -1.7100e+02,\n",
      "         -2.0200e+02, -8.7500e+00,  3.1800e+02, -1.5900e+02,  1.0700e+02,\n",
      "          1.6800e+02,  2.5400e+02,  1.7625e+01, -1.6500e+02,  8.6000e+01,\n",
      "          3.8000e+01,  1.4100e+02,  2.0200e+02, -3.0600e+02, -2.0900e+02,\n",
      "         -1.4600e+02,  6.0800e+02]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Add bias to the result\n",
    "matrix_bias = F.linear(random_hidden_state, random_int8.to(random_hidden_state.dtype)) * random_scale + bias\n",
    "print(\"Matrix Multiplication with scale and bias: \", matrix_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement all of this together into a forward function wieght_8bits_activation_16_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scale, bias=None):\n",
    "  # Cast weight to input type\n",
    "  casted_weight = weight.to(input.dtype)\n",
    "  # Perform matrix multiplication\n",
    "  matrix_mul = F.linear(input, casted_weight) * scale\n",
    "  # Add bias\n",
    "  if bias is not None:\n",
    "    matrix_mul += bias\n",
    "  return matrix_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With bias: \n",
      "\n",
      " tensor([[-7.1500e+01, -3.5000e+01,  2.5300e+02,  1.2900e+02, -1.7773e-01,\n",
      "          4.7750e+01, -6.3250e+01, -4.5200e+02,  3.0800e+02, -1.0500e+02,\n",
      "         -5.6000e+01, -1.5500e+02,  9.8000e+01,  3.7969e+00, -1.7100e+02,\n",
      "         -2.0200e+02, -8.7500e+00,  3.1800e+02, -1.5900e+02,  1.0700e+02,\n",
      "          1.6800e+02,  2.5400e+02,  1.7625e+01, -1.6500e+02,  8.6000e+01,\n",
      "          3.8000e+01,  1.4100e+02,  2.0200e+02, -3.0600e+02, -2.0900e+02,\n",
      "         -1.4600e+02,  6.0800e+02]], dtype=torch.bfloat16)\n",
      "Without bias: \n",
      "\n",
      " tensor([[-7.1000e+01, -3.4000e+01,  2.5400e+02,  1.2900e+02,  4.1211e-01,\n",
      "          4.8000e+01, -6.3000e+01, -4.5200e+02,  3.0800e+02, -1.0500e+02,\n",
      "         -5.8250e+01, -1.5400e+02,  9.7500e+01,  5.9062e+00, -1.7000e+02,\n",
      "         -2.0400e+02, -8.8125e+00,  3.1800e+02, -1.6000e+02,  1.0650e+02,\n",
      "          1.6800e+02,  2.5300e+02,  1.6250e+01, -1.6500e+02,  8.8000e+01,\n",
      "          3.8250e+01,  1.4000e+02,  2.0200e+02, -3.0600e+02, -2.0900e+02,\n",
      "         -1.4400e+02,  6.0800e+02]], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"With bias: \\n\\n\", w8_a16_forward(random_int8, random_hidden_state, random_scale, bias))\n",
    "print(\"Without bias: \\n\\n\", w8_a16_forward(random_int8, random_hidden_state, random_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it all together to a class\n",
    "\n",
    "Note:\n",
    "- This is how the `init` is of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
    "```Python\n",
    "def __init__(self, in_features, out_features, bias=True,\n",
    "             device=None, dtype=None)\n",
    "\n",
    "\n",
    "The below code would fail, because pytorch doesnt support yet, computation of gradient on int8 which nn.parameter will try to perform then, result in error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running this will result in an error\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n",
    "                                     ).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    \n",
    "    W8A16LinearLayer(1, 1)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct class implementation\n",
    "We can bypass the gradient by storing the int8 weight as a buffer\n",
    "Since we're not interesting in the training and only doing simple inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer( \n",
    "            \"int8_weights\",\n",
    "            torch.randint(-128, 127, (out_features, in_features)).to(torch.int8)\n",
    "        )\n",
    "\n",
    "        self.register_buffer( \n",
    "            \"scale\",\n",
    "            torch.randn((out_features), dtype=dtype)\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer( \n",
    "                \"bias\",\n",
    "                torch.randn((1, out_features), dtype=dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    # Add quantizer\n",
    "    def quantize(self, weights):\n",
    "        # cast weight to float32\n",
    "        w_fp32 = weights.to(torch.float32)\n",
    "        # get the scale by getting absolute max value of the the last dimension\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n",
    "        self.int8_weights = int8_weights\n",
    "\n",
    "        self.scale = scales\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, input, self.scale, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16])\n",
      "torch.Size([32])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# try that out with dummy instance\n",
    "module = W8A16LinearLayer(16, 32)\n",
    "dummy_hidden_state = torch.randn((1, 6, 16))\n",
    "print(module.int8_weights.shape)\n",
    "print(module.scale.shape)\n",
    "print(module(dummy_hidden_state).dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before:  tensor([[  51,   55,   35,  -89],\n",
      "        [  94,    3,   47,   27],\n",
      "        [  37,   44,   73,  117],\n",
      "        [   8,  -88,  -36,  -42],\n",
      "        [-111,  -25,   68, -107],\n",
      "        [ -30,   -2,   -3,   48],\n",
      "        [ -46,   28,    2, -102],\n",
      "        [ 108,  112, -112, -115]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "module = W8A16LinearLayer(4, 8)\n",
    "print(\"Weights before: \", module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after:  tensor([[  71,   21,  127,  120,  -75,   -4, -102,   55],\n",
      "        [ -23,   -2,  -54,   25,  -11,   30, -127,    5],\n",
      "        [ -59,  -77,   14,   -6,  -13, -127,   59,   28],\n",
      "        [  -4,  -22,  127,  -31,    1,   41,  -23, -125]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "#pass in dummy weights as original weights andquantize them\n",
    "# we can see the weighst are between -128 and 127\n",
    "random_matrix = torch.randn((4, 8))\n",
    "q_weights = module.quantize(random_matrix)\n",
    "print(\"Weights after: \", module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scales:  tensor([0.0109, 0.0199, 0.0205, 0.0117])\n",
      "Scale shape:  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# look at the scales\n",
    "print(\"Scales: \", module.scale)\n",
    "print(\"Scale shape: \", module.scale.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale shape after unsqueeze:  torch.Size([4, 1])\n",
      "Weights shape:  torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "# let's multiply our unquantized weights with the scale\n",
    "# make sure the scale is broadcasted correctly\n",
    "scale = module.scale.unsqueeze(1)\n",
    "print(\"Scale shape after unsqueeze: \", scale.shape)\n",
    "print(\"Weights shape: \", random_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights * scale:  tensor([[ 0.7711,  0.2281,  1.3792,  1.3032, -0.8145, -0.0434, -1.1077,  0.5973],\n",
      "        [-0.4581, -0.0398, -1.0754,  0.4979, -0.2191,  0.5975, -2.5292,  0.0996],\n",
      "        [-1.2076, -1.5761,  0.2866, -0.1228, -0.2661, -2.5995,  1.2076,  0.5731],\n",
      "        [-0.0469, -0.2578,  1.4880, -0.3632,  0.0117,  0.4804, -0.2695, -1.4646]])\n",
      "Comparing with random matrix:  tensor([[ 0.7718,  0.2334,  1.3792,  1.3077, -0.8145, -0.0439, -1.1104,  0.5937],\n",
      "        [-0.4559, -0.0325, -1.0828,  0.5024, -0.2117,  0.5930, -2.5292,  0.0959],\n",
      "        [-1.2057, -1.5770,  0.2922, -0.1297, -0.2734, -2.5995,  1.2076,  0.5824],\n",
      "        [-0.0484, -0.2610,  1.4880, -0.3655,  0.0063,  0.4819, -0.2655, -1.4615]])\n"
     ]
    }
   ],
   "source": [
    "# since now they're multipyable, let's multiply them\n",
    "q_weights = module.int8_weights * scale\n",
    "print(\"Weights * scale: \",q_weights)\n",
    "print(\"Comparing with random matrix: \", random_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute quantization error\n",
    "def quantization_error(original, quantized):\n",
    "    error_matrix = original - quantized\n",
    "    mean_error = error_matrix.abs().mean()\n",
    "    return error_matrix, mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization error:  tensor([[ 7.7575e-04,  5.2895e-03,  0.0000e+00,  4.4931e-03,  3.6538e-05,\n",
      "         -4.6511e-04, -2.6394e-03, -3.6582e-03],\n",
      "        [ 2.1196e-03,  7.3543e-03, -7.4043e-03,  4.5610e-03,  7.3683e-03,\n",
      "         -4.4237e-03,  0.0000e+00, -3.6940e-03],\n",
      "        [ 1.9451e-03, -9.8431e-04,  5.6031e-03, -6.9324e-03, -7.2791e-03,\n",
      "          0.0000e+00,  8.7023e-06,  9.2616e-03],\n",
      "        [-1.5544e-03, -3.1916e-03,  0.0000e+00, -2.2379e-03, -5.4338e-03,\n",
      "          1.5437e-03,  3.9859e-03,  3.1277e-03]])\n",
      "Mean error:  tensor(0.0034)\n"
     ]
    }
   ],
   "source": [
    "error_matrix, mean_error =  quantization_error(random_matrix, q_weights)\n",
    "print(\"Quantization error: \", error_matrix)\n",
    "print(\"Mean error: \", mean_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
