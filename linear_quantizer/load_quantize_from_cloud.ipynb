{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4-D - Building your own Quantizer: Load your Quantized Weights from Hugging Face Hub\n",
    "\n",
    "In this lesson, you will learn memory efficient model loading.\n",
    "\n",
    "The goal for this is, we can use any big machine to load a large model and quantize it\n",
    "Then push the quantzied version so you can load it in your small machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'W8A16LinearLayerDtype' from 'helpers' (/Users/tango.tew/Library/CloudStorage/OneDrive-EY/Documents/repos/AI-Projects/model-quantization/linear_quantizer/helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m W8A16LinearLayer, W8A16LinearLayerDtype, replace_linear_with_target_and_quantize, replace_linear_with_target\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'W8A16LinearLayerDtype' from 'helpers' (/Users/tango.tew/Library/CloudStorage/OneDrive-EY/Documents/repos/AI-Projects/model-quantization/linear_quantizer/helpers.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from helpers import W8A16LinearLayer, W8A16LinearLayerDtype, replace_linear_with_target_and_quantize, replace_linear_with_target\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/opt-125m\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize and Save your weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): W8A16LinearLayer()\n",
       "            (v_proj): W8A16LinearLayer()\n",
       "            (q_proj): W8A16LinearLayer()\n",
       "            (out_proj): W8A16LinearLayer()\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A16LinearLayer()\n",
       "          (fc2): W8A16LinearLayer()\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"lm_head\"])\n",
    "\n",
    "# Make sure the model is quantized\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model weights into dictionary\n",
    "model_dict = model.state_dict()\n",
    "save_path = \"../models/quantized/fb_125m_quantized_state_dict.pth\"\n",
    "torch.save(model_dict, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Online Repository to push your quantized weights\n",
    "In this case we're going to use HuggingFace\n",
    "\n",
    "The below code is for demonstration purposes only.\n",
    "\n",
    "You'll need your own Hugging Face username in order for it to run.\n",
    "You'll add your usernmae in YOUR_HF_USERNAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fb_125m_quantized_state_dict.pth: 100%|██████████| 166M/166M [00:25<00:00, 6.54MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tew9/opt-125m-quantized-deeplearningai/commit/a2599c2c1c9d3016cad31e52b4ab124507ea62be', commit_message='Upload opt_125m_quantized_state_dict.pth with huggingface_hub', commit_description='', oid='a2599c2c1c9d3016cad31e52b4ab124507ea62be', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# get the api token and the username from the environment variables\n",
    "USERNAME = os.getenv(\"USERNAME\")\n",
    "# Get the Hugging Face API token from the environment variable\n",
    "api_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "\n",
    "if not api_token:\n",
    "  raise ValueError(\"No API token found. Please set HUGGINGFACE_HUB_TOKEN in your .env file.\")\n",
    "\n",
    "if not USERNAME:\n",
    "  raise ValueError(\"No username found. Please set USERNAME in your .env file.\")\n",
    "\n",
    "\n",
    "repo_id = f\"{USERNAME}/opt-125m-quantized-deeplearningai\"\n",
    "\n",
    "# instantiate the HfApi class\n",
    "api = HfApi()\n",
    "\n",
    "# create a new repository if not already created\n",
    "create_repo(repo_id=repo_id, token=api_token, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "api.upload_file(\n",
    "  path_or_fileobj=save_path,\n",
    "  path_in_repo=\"opt_125m_quantized_state_dict.pth\",\n",
    "  repo_id=repo_id,\n",
    "  token=api_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Quantized Weights from huggingface\n",
    "load the meta device(pytorch skeleton of the model - to be used to load the quantized weights from the hugging face) to make the model architecture match the one on the hugging face\n",
    "\n",
    "- We use meta device to load the skeleton of the model config(architecture) but without loading the weights themselves which save us memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(50272, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(2050, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072, 768), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(3072,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768, 3072), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(..., device='meta', size=(768,), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "with torch.device(\"meta\"):\n",
    "    model_arch = OPTForCausalLM(config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# load quantized weights from the hugging face\n",
    "\n",
    "for param in model_arch.parameters():\n",
    "  print(param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the tensor do not have weights at all (...), but we have the entire model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from helpers import  W8A16LinearLayer\n",
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output\n",
    "\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scale\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "        int8_weights = torch.round(weights / scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights.data = int8_weights\n",
    "        self.scale.data = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scale, self.bias)\n",
    "\n",
    "def replace_linear_with_target(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and name not in module_name_to_exclude:\n",
    "            old_bias = child.bias\n",
    "            \n",
    "            new_module = target_class(child.in_features, \n",
    "                                      child.out_features, \n",
    "                                      old_bias is not None, \n",
    "                                      child.weight.dtype)\n",
    "            new_module.to(child.weight.device)\n",
    "            \n",
    "            setattr(module, name, new_module)\n",
    "            \n",
    "            # Copy weights and bias (if exists)\n",
    "            new_module.quantize(child.weight.data)\n",
    "            if old_bias is not None:\n",
    "                new_module.bias.data = old_bias.data.clone().to(new_module.bias.dtype)\n",
    "                \n",
    "        else:\n",
    "            replace_linear_with_target(child, target_class, module_name_to_exclude)\n",
    "\n",
    "# # Example usage\n",
    "# # from transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# # model_id = \"facebook/opt-125m\"\n",
    "# # config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# # with torch.device(\"meta\"):\n",
    "# #     model_arch = OPTForCausalLM(config)\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# replace_linear_with_target(model_arch, W8A16LinearLayer, [\"lm_head\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): W8A16LinearLayer()\n",
       "            (v_proj): W8A16LinearLayer()\n",
       "            (q_proj): W8A16LinearLayer()\n",
       "            (out_proj): W8A16LinearLayer()\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A16LinearLayerDtype()\n",
       "          (fc2): W8A16LinearLayerDtype()\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-11): 11 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): W8A16LinearLayerDtype()\n",
       "            (v_proj): W8A16LinearLayerDtype()\n",
       "            (q_proj): W8A16LinearLayerDtype()\n",
       "            (out_proj): W8A16LinearLayerDtype()\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): W8A16LinearLayerDtype()\n",
       "          (fc2): W8A16LinearLayerDtype()\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_linear_with_target(model, W8A16LinearLayer, [\"lm_head\"])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model weights from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OPTForCausalLM:\n\tMissing key(s) in state_dict: \"model.decoder.layers.0.fc1.scales\", \"model.decoder.layers.0.fc2.scales\", \"model.decoder.layers.1.self_attn.k_proj.scales\", \"model.decoder.layers.1.self_attn.v_proj.scales\", \"model.decoder.layers.1.self_attn.q_proj.scales\", \"model.decoder.layers.1.self_attn.out_proj.scales\", \"model.decoder.layers.1.fc1.scales\", \"model.decoder.layers.1.fc2.scales\", \"model.decoder.layers.2.self_attn.k_proj.scales\", \"model.decoder.layers.2.self_attn.v_proj.scales\", \"model.decoder.layers.2.self_attn.q_proj.scales\", \"model.decoder.layers.2.self_attn.out_proj.scales\", \"model.decoder.layers.2.fc1.scales\", \"model.decoder.layers.2.fc2.scales\", \"model.decoder.layers.3.self_attn.k_proj.scales\", \"model.decoder.layers.3.self_attn.v_proj.scales\", \"model.decoder.layers.3.self_attn.q_proj.scales\", \"model.decoder.layers.3.self_attn.out_proj.scales\", \"model.decoder.layers.3.fc1.scales\", \"model.decoder.layers.3.fc2.scales\", \"model.decoder.layers.4.self_attn.k_proj.scales\", \"model.decoder.layers.4.self_attn.v_proj.scales\", \"model.decoder.layers.4.self_attn.q_proj.scales\", \"model.decoder.layers.4.self_attn.out_proj.scales\", \"model.decoder.layers.4.fc1.scales\", \"model.decoder.layers.4.fc2.scales\", \"model.decoder.layers.5.self_attn.k_proj.scales\", \"model.decoder.layers.5.self_attn.v_proj.scales\", \"model.decoder.layers.5.self_attn.q_proj.scales\", \"model.decoder.layers.5.self_attn.out_proj.scales\", \"model.decoder.layers.5.fc1.scales\", \"model.decoder.layers.5.fc2.scales\", \"model.decoder.layers.6.self_attn.k_proj.scales\", \"model.decoder.layers.6.self_attn.v_proj.scales\", \"model.decoder.layers.6.self_attn.q_proj.scales\", \"model.decoder.layers.6.self_attn.out_proj.scales\", \"model.decoder.layers.6.fc1.scales\", \"model.decoder.layers.6.fc2.scales\", \"model.decoder.layers.7.self_attn.k_proj.scales\", \"model.decoder.layers.7.self_attn.v_proj.scales\", \"model.decoder.layers.7.self_attn.q_proj.scales\", \"model.decoder.layers.7.self_attn.out_proj.scales\", \"model.decoder.layers.7.fc1.scales\", \"model.decoder.layers.7.fc2.scales\", \"model.decoder.layers.8.self_attn.k_proj.scales\", \"model.decoder.layers.8.self_attn.v_proj.scales\", \"model.decoder.layers.8.self_attn.q_proj.scales\", \"model.decoder.layers.8.self_attn.out_proj.scales\", \"model.decoder.layers.8.fc1.scales\", \"model.decoder.layers.8.fc2.scales\", \"model.decoder.layers.9.self_attn.k_proj.scales\", \"model.decoder.layers.9.self_attn.v_proj.scales\", \"model.decoder.layers.9.self_attn.q_proj.scales\", \"model.decoder.layers.9.self_attn.out_proj.scales\", \"model.decoder.layers.9.fc1.scales\", \"model.decoder.layers.9.fc2.scales\", \"model.decoder.layers.10.self_attn.k_proj.scales\", \"model.decoder.layers.10.self_attn.v_proj.scales\", \"model.decoder.layers.10.self_attn.q_proj.scales\", \"model.decoder.layers.10.self_attn.out_proj.scales\", \"model.decoder.layers.10.fc1.scales\", \"model.decoder.layers.10.fc2.scales\", \"model.decoder.layers.11.self_attn.k_proj.scales\", \"model.decoder.layers.11.self_attn.v_proj.scales\", \"model.decoder.layers.11.self_attn.q_proj.scales\", \"model.decoder.layers.11.self_attn.out_proj.scales\", \"model.decoder.layers.11.fc1.scales\", \"model.decoder.layers.11.fc2.scales\". \n\tUnexpected key(s) in state_dict: \"model.decoder.layers.0.fc1.scale\", \"model.decoder.layers.0.fc2.scale\", \"model.decoder.layers.1.self_attn.k_proj.scale\", \"model.decoder.layers.1.self_attn.v_proj.scale\", \"model.decoder.layers.1.self_attn.q_proj.scale\", \"model.decoder.layers.1.self_attn.out_proj.scale\", \"model.decoder.layers.1.fc1.scale\", \"model.decoder.layers.1.fc2.scale\", \"model.decoder.layers.2.self_attn.k_proj.scale\", \"model.decoder.layers.2.self_attn.v_proj.scale\", \"model.decoder.layers.2.self_attn.q_proj.scale\", \"model.decoder.layers.2.self_attn.out_proj.scale\", \"model.decoder.layers.2.fc1.scale\", \"model.decoder.layers.2.fc2.scale\", \"model.decoder.layers.3.self_attn.k_proj.scale\", \"model.decoder.layers.3.self_attn.v_proj.scale\", \"model.decoder.layers.3.self_attn.q_proj.scale\", \"model.decoder.layers.3.self_attn.out_proj.scale\", \"model.decoder.layers.3.fc1.scale\", \"model.decoder.layers.3.fc2.scale\", \"model.decoder.layers.4.self_attn.k_proj.scale\", \"model.decoder.layers.4.self_attn.v_proj.scale\", \"model.decoder.layers.4.self_attn.q_proj.scale\", \"model.decoder.layers.4.self_attn.out_proj.scale\", \"model.decoder.layers.4.fc1.scale\", \"model.decoder.layers.4.fc2.scale\", \"model.decoder.layers.5.self_attn.k_proj.scale\", \"model.decoder.layers.5.self_attn.v_proj.scale\", \"model.decoder.layers.5.self_attn.q_proj.scale\", \"model.decoder.layers.5.self_attn.out_proj.scale\", \"model.decoder.layers.5.fc1.scale\", \"model.decoder.layers.5.fc2.scale\", \"model.decoder.layers.6.self_attn.k_proj.scale\", \"model.decoder.layers.6.self_attn.v_proj.scale\", \"model.decoder.layers.6.self_attn.q_proj.scale\", \"model.decoder.layers.6.self_attn.out_proj.scale\", \"model.decoder.layers.6.fc1.scale\", \"model.decoder.layers.6.fc2.scale\", \"model.decoder.layers.7.self_attn.k_proj.scale\", \"model.decoder.layers.7.self_attn.v_proj.scale\", \"model.decoder.layers.7.self_attn.q_proj.scale\", \"model.decoder.layers.7.self_attn.out_proj.scale\", \"model.decoder.layers.7.fc1.scale\", \"model.decoder.layers.7.fc2.scale\", \"model.decoder.layers.8.self_attn.k_proj.scale\", \"model.decoder.layers.8.self_attn.v_proj.scale\", \"model.decoder.layers.8.self_attn.q_proj.scale\", \"model.decoder.layers.8.self_attn.out_proj.scale\", \"model.decoder.layers.8.fc1.scale\", \"model.decoder.layers.8.fc2.scale\", \"model.decoder.layers.9.self_attn.k_proj.scale\", \"model.decoder.layers.9.self_attn.v_proj.scale\", \"model.decoder.layers.9.self_attn.q_proj.scale\", \"model.decoder.layers.9.self_attn.out_proj.scale\", \"model.decoder.layers.9.fc1.scale\", \"model.decoder.layers.9.fc2.scale\", \"model.decoder.layers.10.self_attn.k_proj.scale\", \"model.decoder.layers.10.self_attn.v_proj.scale\", \"model.decoder.layers.10.self_attn.q_proj.scale\", \"model.decoder.layers.10.self_attn.out_proj.scale\", \"model.decoder.layers.10.fc1.scale\", \"model.decoder.layers.10.fc2.scale\", \"model.decoder.layers.11.self_attn.k_proj.scale\", \"model.decoder.layers.11.self_attn.v_proj.scale\", \"model.decoder.layers.11.self_attn.q_proj.scale\", \"model.decoder.layers.11.self_attn.out_proj.scale\", \"model.decoder.layers.11.fc1.scale\", \"model.decoder.layers.11.fc2.scale\". \n\tsize mismatch for model.decoder.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1, 3072]).\n\tsize mismatch for model.decoder.layers.0.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m quantized_state_dict \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m quantized_state_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# check the shape of one of the tensors\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Assign the model from the meta device the weight from the quantized state dict\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OPTForCausalLM:\n\tMissing key(s) in state_dict: \"model.decoder.layers.0.fc1.scales\", \"model.decoder.layers.0.fc2.scales\", \"model.decoder.layers.1.self_attn.k_proj.scales\", \"model.decoder.layers.1.self_attn.v_proj.scales\", \"model.decoder.layers.1.self_attn.q_proj.scales\", \"model.decoder.layers.1.self_attn.out_proj.scales\", \"model.decoder.layers.1.fc1.scales\", \"model.decoder.layers.1.fc2.scales\", \"model.decoder.layers.2.self_attn.k_proj.scales\", \"model.decoder.layers.2.self_attn.v_proj.scales\", \"model.decoder.layers.2.self_attn.q_proj.scales\", \"model.decoder.layers.2.self_attn.out_proj.scales\", \"model.decoder.layers.2.fc1.scales\", \"model.decoder.layers.2.fc2.scales\", \"model.decoder.layers.3.self_attn.k_proj.scales\", \"model.decoder.layers.3.self_attn.v_proj.scales\", \"model.decoder.layers.3.self_attn.q_proj.scales\", \"model.decoder.layers.3.self_attn.out_proj.scales\", \"model.decoder.layers.3.fc1.scales\", \"model.decoder.layers.3.fc2.scales\", \"model.decoder.layers.4.self_attn.k_proj.scales\", \"model.decoder.layers.4.self_attn.v_proj.scales\", \"model.decoder.layers.4.self_attn.q_proj.scales\", \"model.decoder.layers.4.self_attn.out_proj.scales\", \"model.decoder.layers.4.fc1.scales\", \"model.decoder.layers.4.fc2.scales\", \"model.decoder.layers.5.self_attn.k_proj.scales\", \"model.decoder.layers.5.self_attn.v_proj.scales\", \"model.decoder.layers.5.self_attn.q_proj.scales\", \"model.decoder.layers.5.self_attn.out_proj.scales\", \"model.decoder.layers.5.fc1.scales\", \"model.decoder.layers.5.fc2.scales\", \"model.decoder.layers.6.self_attn.k_proj.scales\", \"model.decoder.layers.6.self_attn.v_proj.scales\", \"model.decoder.layers.6.self_attn.q_proj.scales\", \"model.decoder.layers.6.self_attn.out_proj.scales\", \"model.decoder.layers.6.fc1.scales\", \"model.decoder.layers.6.fc2.scales\", \"model.decoder.layers.7.self_attn.k_proj.scales\", \"model.decoder.layers.7.self_attn.v_proj.scales\", \"model.decoder.layers.7.self_attn.q_proj.scales\", \"model.decoder.layers.7.self_attn.out_proj.scales\", \"model.decoder.layers.7.fc1.scales\", \"model.decoder.layers.7.fc2.scales\", \"model.decoder.layers.8.self_attn.k_proj.scales\", \"model.decoder.layers.8.self_attn.v_proj.scales\", \"model.decoder.layers.8.self_attn.q_proj.scales\", \"model.decoder.layers.8.self_attn.out_proj.scales\", \"model.decoder.layers.8.fc1.scales\", \"model.decoder.layers.8.fc2.scales\", \"model.decoder.layers.9.self_attn.k_proj.scales\", \"model.decoder.layers.9.self_attn.v_proj.scales\", \"model.decoder.layers.9.self_attn.q_proj.scales\", \"model.decoder.layers.9.self_attn.out_proj.scales\", \"model.decoder.layers.9.fc1.scales\", \"model.decoder.layers.9.fc2.scales\", \"model.decoder.layers.10.self_attn.k_proj.scales\", \"model.decoder.layers.10.self_attn.v_proj.scales\", \"model.decoder.layers.10.self_attn.q_proj.scales\", \"model.decoder.layers.10.self_attn.out_proj.scales\", \"model.decoder.layers.10.fc1.scales\", \"model.decoder.layers.10.fc2.scales\", \"model.decoder.layers.11.self_attn.k_proj.scales\", \"model.decoder.layers.11.self_attn.v_proj.scales\", \"model.decoder.layers.11.self_attn.q_proj.scales\", \"model.decoder.layers.11.self_attn.out_proj.scales\", \"model.decoder.layers.11.fc1.scales\", \"model.decoder.layers.11.fc2.scales\". \n\tUnexpected key(s) in state_dict: \"model.decoder.layers.0.fc1.scale\", \"model.decoder.layers.0.fc2.scale\", \"model.decoder.layers.1.self_attn.k_proj.scale\", \"model.decoder.layers.1.self_attn.v_proj.scale\", \"model.decoder.layers.1.self_attn.q_proj.scale\", \"model.decoder.layers.1.self_attn.out_proj.scale\", \"model.decoder.layers.1.fc1.scale\", \"model.decoder.layers.1.fc2.scale\", \"model.decoder.layers.2.self_attn.k_proj.scale\", \"model.decoder.layers.2.self_attn.v_proj.scale\", \"model.decoder.layers.2.self_attn.q_proj.scale\", \"model.decoder.layers.2.self_attn.out_proj.scale\", \"model.decoder.layers.2.fc1.scale\", \"model.decoder.layers.2.fc2.scale\", \"model.decoder.layers.3.self_attn.k_proj.scale\", \"model.decoder.layers.3.self_attn.v_proj.scale\", \"model.decoder.layers.3.self_attn.q_proj.scale\", \"model.decoder.layers.3.self_attn.out_proj.scale\", \"model.decoder.layers.3.fc1.scale\", \"model.decoder.layers.3.fc2.scale\", \"model.decoder.layers.4.self_attn.k_proj.scale\", \"model.decoder.layers.4.self_attn.v_proj.scale\", \"model.decoder.layers.4.self_attn.q_proj.scale\", \"model.decoder.layers.4.self_attn.out_proj.scale\", \"model.decoder.layers.4.fc1.scale\", \"model.decoder.layers.4.fc2.scale\", \"model.decoder.layers.5.self_attn.k_proj.scale\", \"model.decoder.layers.5.self_attn.v_proj.scale\", \"model.decoder.layers.5.self_attn.q_proj.scale\", \"model.decoder.layers.5.self_attn.out_proj.scale\", \"model.decoder.layers.5.fc1.scale\", \"model.decoder.layers.5.fc2.scale\", \"model.decoder.layers.6.self_attn.k_proj.scale\", \"model.decoder.layers.6.self_attn.v_proj.scale\", \"model.decoder.layers.6.self_attn.q_proj.scale\", \"model.decoder.layers.6.self_attn.out_proj.scale\", \"model.decoder.layers.6.fc1.scale\", \"model.decoder.layers.6.fc2.scale\", \"model.decoder.layers.7.self_attn.k_proj.scale\", \"model.decoder.layers.7.self_attn.v_proj.scale\", \"model.decoder.layers.7.self_attn.q_proj.scale\", \"model.decoder.layers.7.self_attn.out_proj.scale\", \"model.decoder.layers.7.fc1.scale\", \"model.decoder.layers.7.fc2.scale\", \"model.decoder.layers.8.self_attn.k_proj.scale\", \"model.decoder.layers.8.self_attn.v_proj.scale\", \"model.decoder.layers.8.self_attn.q_proj.scale\", \"model.decoder.layers.8.self_attn.out_proj.scale\", \"model.decoder.layers.8.fc1.scale\", \"model.decoder.layers.8.fc2.scale\", \"model.decoder.layers.9.self_attn.k_proj.scale\", \"model.decoder.layers.9.self_attn.v_proj.scale\", \"model.decoder.layers.9.self_attn.q_proj.scale\", \"model.decoder.layers.9.self_attn.out_proj.scale\", \"model.decoder.layers.9.fc1.scale\", \"model.decoder.layers.9.fc2.scale\", \"model.decoder.layers.10.self_attn.k_proj.scale\", \"model.decoder.layers.10.self_attn.v_proj.scale\", \"model.decoder.layers.10.self_attn.q_proj.scale\", \"model.decoder.layers.10.self_attn.out_proj.scale\", \"model.decoder.layers.10.fc1.scale\", \"model.decoder.layers.10.fc2.scale\", \"model.decoder.layers.11.self_attn.k_proj.scale\", \"model.decoder.layers.11.self_attn.v_proj.scale\", \"model.decoder.layers.11.self_attn.q_proj.scale\", \"model.decoder.layers.11.self_attn.out_proj.scale\", \"model.decoder.layers.11.fc1.scale\", \"model.decoder.layers.11.fc2.scale\". \n\tsize mismatch for model.decoder.layers.0.self_attn.k_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.v_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.q_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768]).\n\tsize mismatch for model.decoder.layers.0.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1, 3072]).\n\tsize mismatch for model.decoder.layers.0.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1, 768])."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the quantized state dict and cache it\n",
    "state_dict_cache_path = hf_hub_download(repo_id, \"opt_125m_quantized_state_dict.pth\")\n",
    "# Load the quantized state dict\n",
    "quantized_state_dict = torch.load(state_dict_cache_path)\n",
    "\n",
    "# Remove incompatible keys\n",
    "quantized_state_dict = {k.replace('scales', 'scale'): v for k, v in quantized_state_dict.items()}\n",
    "\n",
    "# check the shape of one of the tensors\n",
    "# Assign the model from the meta device the weight from the quantized state dict\n",
    "model.load_state_dict(quantized_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
