{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_linear_W8A32_without_bias(input, quantized_w, scale_w, zeropoint_w):\n",
    "  # make sure the input is float32\n",
    "  assert input.dtype == torch.float32, \"Input should be float32\"\n",
    "  # make sure the quanted weights are int8\n",
    "  assert quantized_w.dtype == torch.int8, \"Weights should be int8\"\n",
    "  # dequantize the weights\n",
    "  dequantized_weights = quantized_w.float() * scale_w + zeropoint_w\n",
    "  #make inference using linear layer\n",
    "  output = torch.nn.functional.linear(input, dequantized_weights)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([1., 2., 3.]) torch.FloatTensor\n",
      "quantized_w:  tensor([[-2.0000, -1.1300,  0.4200],\n",
      "        [ 1.2000,  3.1000,  2.1000],\n",
      "        [ 0.1000,  0.3000, -0.2000]])\n"
     ]
    }
   ],
   "source": [
    "# let's test our inference function on random input\n",
    "input  = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "print(\"input: \", input, input.type())\n",
    "weights = torch.tensor([[-2, -1.13, 0.42],\n",
    "                            [1.2, 3.1, 2.1],\n",
    "                            [0.1, 0.3, -0.2]])\n",
    "print(\"quantized_w: \", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_w:  0.024409448067972978\n",
      "quantized_w:  tensor([[-82, -46,  17],\n",
      "        [ 49, 127,  86],\n",
      "        [  4,  12,  -8]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "from helpers import linear_q_symmetric\n",
    "quantized_w, scale_w = linear_q_symmetric(weights)\n",
    "print(\"scale_w: \", scale_w)\n",
    "print(\"quantized_w: \", quantized_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantized inference output:  tensor([-3.0024, 13.6937,  0.0976])\n"
     ]
    }
   ],
   "source": [
    "output = quantize_linear_W8A32_without_bias(input, quantized_w, scale_w, 0)\n",
    "print(\"quantized inference output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floating point inference output:  tensor([-3.0000, 13.7000,  0.1000])\n"
     ]
    }
   ],
   "source": [
    "# let's compare the output with the original floating  point inference\n",
    "output = torch.nn.functional.linear(input, weights)\n",
    "print(\"floating point inference output: \", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
