{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L4-B - Building your own Quantizer: Replace PyTorch layers with Quantized LayersÂ¶\n",
    "In this lesson, you will learn about the quantization pipline using your own 8-bit quantizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Quantization Pipeline\n",
    "Replace all of the torch.nn.Linear layers with the W8A16LinearLayer layer.\n",
    "Call quantize on the linear layers using the original weights.\n",
    "2.1 - Model In-place Linear Layer Replacement\n",
    "Implement replace_linear_with_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over torch.nn.Module children and replace them with a new module\n",
    "def replace_linear_with_target(module, target_class, module_names_to_exclude):\n",
    "  '''\n",
    "  Replace all linear layers in a module with a target class.\n",
    "\n",
    "  Parameters:\n",
    "  - module: The module containing the linear layers to be replaced.\n",
    "  - target_class: The target class to replace the linear layers with.\n",
    "  - module_names_to_exclude: A list of module names to exclude from replacement.\n",
    "\n",
    "  Returns:\n",
    "  - The modified module with linear layers replaced by the target class.\n",
    "  '''\n",
    "  for name, child in module.named_children():\n",
    "    if isinstance(child, torch.nn.Linear) and not any([x ==name for x in module_names_to_exclude]):\n",
    "      # get old module bias\n",
    "      old_bias = child.bias\n",
    "\n",
    "      # create new module \n",
    "      new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "\n",
    "      # replace current module name with new module\n",
    "      setattr(module, name, new_module)\n",
    "      if old_bias is not None:\n",
    "        # if old module had bias, set the bias of new module to the old bias\n",
    "        getattr(module, name).bias.data = old_bias\n",
    "    else:\n",
    "      # Recursively apply the function to the child module for nested modules\n",
    "      replace_linear_with_target(child, target_class, module_names_to_exclude)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model\n",
    "class DummyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(1,1)\n",
    "    # Try With Bias\n",
    "    self.linear_1 = nn.Linear(1, 1)\n",
    "    # Try Without Bias\n",
    "    self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "    # Linear Model predition head\n",
    "    self.lm_head = nn.Linear(1, 1, bias=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 before replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (linear_2): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "Model 2 before replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (linear_2): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy models, one with excluded layers and one with replacing all layers\n",
    "model_1 = DummyModel() # exclude features\n",
    "model_2 = DummyModel() # replace all layers\n",
    "print(\"Model 1 before replacing it with the quantizable module: \", model_1)\n",
    "print(\"Model 2 before replacing it with the quantizable module: \", model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to replace our model features excluding lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 after replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(\"Model 1 after replacing it with the quantizable module: \", model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try to replace all the linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 after replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_target(model_2, W8A16LinearLayer, [])\n",
    "print(\"Model 2 after replacing it with the quantizable module: \", model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Quantize the new module once we replace the old module with the new module\n",
    "Let's refine the function above to add quantization for all the replaced layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over torch.nn.Module children and replace them with a new module\n",
    "def replace_linear_with_target_and_quantize(module, target_class, module_names_to_exclude):\n",
    "  '''\n",
    "  Replace all linear layers in a module with a target class.\n",
    "\n",
    "  Parameters:\n",
    "  - module: The module containing the linear layers to be replaced.\n",
    "  - target_class: The target class to replace the linear layers with.\n",
    "  - module_names_to_exclude: A list of module names to exclude from replacement.\n",
    "\n",
    "  Returns:\n",
    "  - The modified module with linear layers replaced by the target class.\n",
    "  '''\n",
    "  for name, child in module.named_children():\n",
    "    if isinstance(child, torch.nn.Linear) and not any([x ==name for x in module_names_to_exclude]):\n",
    "      # get old module bias\n",
    "      old_bias = child.bias\n",
    "      # retrieve the old weight\n",
    "      old_weight = child.weight\n",
    "\n",
    "      # create new module \n",
    "      new_module = target_class(child.in_features, child.out_features, old_bias is not None, child.weight.dtype)\n",
    "\n",
    "      # replace current module name with new module\n",
    "      setattr(module, name, new_module)\n",
    "\n",
    "      # Once the old module is replaced above, we can now set the old weight to the new module\n",
    "      # Get this new module, and quantize it's old weight\n",
    "      getattr(module, name).quantize(old_weight)\n",
    "\n",
    "      if old_bias is not None:\n",
    "        # if old module had bias, set the bias of new module to the old bias\n",
    "        getattr(module, name).bias.data = old_bias\n",
    "    else:\n",
    "      # Recursively apply the function to the child module for nested modules\n",
    "      replace_linear_with_target_and_quantize(child, target_class, module_names_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 before replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (linear_2): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "Model 3 after replacing it with the quantizable module:  DummyModel(\n",
      "  (emb): Embedding(1, 1)\n",
      "  (linear_1): W8A16LinearLayer()\n",
      "  (linear_2): W8A16LinearLayer()\n",
      "  (lm_head): W8A16LinearLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Try it out on another dummy model\n",
    "model_3 = DummyModel() # exclude features\n",
    "print(\"Model 3 before replacing it with the quantizable module: \", model_3)\n",
    "replace_linear_with_target_and_quantize(model_3, W8A16LinearLayer, [])\n",
    "print(\"Model 3 after replacing it with the quantizable module: \", model_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
